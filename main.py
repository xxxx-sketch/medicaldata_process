#!/usr/bin/env python3
# main2.pyä¸»æ–‡ä»¶
"""
Adaptive GCN
"""
import argparse
import sys
import os
import torch
import warnings
from gcn_models import AdaptiveGCN, initialize_model, print_model_summary, count_parameters
from utils import (save_training_plot, visualize_predictions,
                   create_output_directories, print_training_summary)
from data_loader import DataLoader

#æœ‰ä¸€äº›å†²çªé—®é¢˜
os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'
os.environ['OMP_NUM_THREADS'] = '1'

# æŠ‘åˆ¶è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning)

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)

# ç”¨äº†ä¸‰ä¸ªè‡ªå®šä¹‰æ¨¡å—ï¼Œgcn_models, utils, data_loader




def train_adaptive_gcn(model, data, optimizer, criterion, epochs=200):
    """è®­ç»ƒè‡ªé€‚åº”GCNæ¨¡å‹"""
    model.train()
    train_losses = []
    test_accuracies = []

    x, y, edge_index = data
    num_nodes = x.size(0)

    # åˆ’åˆ†è®­ç»ƒæµ‹è¯•é›† (80% è®­ç»ƒ, 20% æµ‹è¯•)
    train_mask = torch.zeros(num_nodes, dtype=torch.bool)
    train_mask[:int(0.8 * num_nodes)] = True
    test_mask = ~train_mask

    # ä½¿ç”¨è·¨å¹³å°çš„è®¡æ—¶æ–¹æ³•
    import time
    start_time = time.time()

    print("ğŸ¯ å¼€å§‹è®­ç»ƒè‡ªé€‚åº”GCN...")
    for epoch in range(epochs):
        optimizer.zero_grad()

        # è‡ªé€‚åº”GCNå‰å‘ä¼ æ’­
        output = model(x)

        loss = criterion(output[train_mask], y[train_mask])
        loss.backward()
        optimizer.step()

        # è¯„ä¼°
        model.eval()
        with torch.no_grad():
            test_output = model(x)
            pred = test_output.argmax(dim=1)
            test_acc = (pred[test_mask] == y[test_mask]).float().mean()

        model.train()
        train_losses.append(loss.item())
        test_accuracies.append(test_acc.item())

        if epoch % 50 == 0 or epoch == epochs - 1:
            print(f'Epoch {epoch:3d} | Loss: {loss.item():.4f} | Test Acc: {test_acc:.4f}')

    training_time = time.time() - start_time

    return train_losses, test_accuracies, training_time


def run_training(data_file=None, epochs=200, lr=0.01, hidden_dim=128):
    """è¿è¡Œè®­ç»ƒæµç¨‹ - ä¸“æ³¨äºç–¾ç—…æ•°æ®"""
    print("=" * 60)
    print("ğŸš€ è‡ªé€‚åº”GCNç–¾ç—…æ•°æ®åˆ†æå¼€å§‹")
    print("=" * 60)
    print(f"ğŸ“Š æ¨¡å‹: AdaptiveGCN")
    print(f"ğŸ”„ è®­ç»ƒè½®æ•°: {epochs}")
    print(f"ğŸ“ˆ å­¦ä¹ ç‡: {lr}")
    print(f"ğŸ§  éšè—å±‚ç»´åº¦: {hidden_dim}")

    # ä½¿ç”¨é»˜è®¤æ•°æ®æ–‡ä»¶è·¯å¾„
    if data_file is None:
        data_file = DataLoader.DEFAULT_DATA_PATH
        print(f"ä½¿ç”¨é»˜è®¤æ•°æ®æ–‡ä»¶: {data_file}")
    else:
        print(f"æ•°æ®æ–‡ä»¶: {data_file}")
    print("=" * 60)

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(data_file):
        print(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
        print("è¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„æˆ–ä½¿ç”¨ --data-file å‚æ•°æŒ‡å®šæ­£ç¡®çš„è·¯å¾„")
        return 0.0

    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)

    # åˆ›å»ºè¾“å‡ºç›®å½•
    create_output_directories()

    # åŠ è½½æ•°æ®
    print("åŠ è½½ç–¾ç—…æ•°æ®...")
    result = DataLoader.load_data(csv_file_path=data_file)
    if result is None:
        print("æ•°æ®åŠ è½½å¤±è´¥")
        return 0.0

    x, y, edge_index, patient_disease_map = result
    input_dim = x.shape[1]  # åŠ¨æ€è·å–åµŒå…¥ç»´åº¦
    num_classes = len(torch.unique(y))

    print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ:")
    print(f"   - èŠ‚ç‚¹ç‰¹å¾ç»´åº¦: {input_dim}")
    print(f"   - èŠ‚ç‚¹æ•°é‡: {x.shape[0]}")
    print(f"   - ç±»åˆ«æ•°é‡: {num_classes}")
    print(f"   - æ ‡ç­¾åˆ†å¸ƒ: {torch.bincount(y).tolist()}")

    # åˆ›å»ºè‡ªé€‚åº”GCNæ¨¡å‹
    print(f"ğŸ› ï¸  åˆ›å»ºè‡ªé€‚åº”GCNæ¨¡å‹...")
    model = AdaptiveGCN(input_dim, hidden_dim, num_classes)
    model = initialize_model(model)

    print_model_summary(model, input_dim)

    # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)
    criterion = torch.nn.NLLLoss()

    # è®­ç»ƒæ¨¡å‹
    train_losses, test_accuracies, training_time = train_adaptive_gcn(
        model, (x, y, edge_index), optimizer, criterion, epochs=epochs
    )

    # æœ€ç»ˆè¯„ä¼°
    model.eval()
    with torch.no_grad():
        final_output = model(x)
        num_nodes = x.size(0)
        train_mask = torch.zeros(num_nodes, dtype=torch.bool)
        train_mask[:int(0.8 * num_nodes)] = True
        test_mask = ~train_mask

        pred = final_output.argmax(dim=1)
        final_acc = (pred[test_mask] == y[test_mask]).float().mean()

        # è®¡ç®—æ•´ä½“å‡†ç¡®ç‡
        overall_acc = (pred == y).float().mean()

    # ä¿å­˜è®­ç»ƒå›¾è¡¨
    save_training_plot(
        train_losses, test_accuracies,
        filename=f'plots/adaptive_gcn_training.png'
    )

    # å¯è§†åŒ–é¢„æµ‹ç»“æœ
    visualize_predictions(
        x, y, pred, test_mask,
        model_name='AdaptiveGCN',
        filename=f'plots/adaptive_gcn_predictions.png'
    )

    # æ‰“å°è®­ç»ƒæ‘˜è¦
    print_training_summary('AdaptiveGCN', final_acc.item(), training_time)
    print(f"ğŸ“Š æ•´ä½“å‡†ç¡®ç‡: {overall_acc.item():.4f}")

    # ä¿å­˜æ¨¡å‹
    os.makedirs('checkpoints', exist_ok=True)
    model_path = f'checkpoints/adaptive_gcn_model.pth'
    torch.save({
        'model_state_dict': model.state_dict(),
        'model_type': 'AdaptiveGCN',
        'final_accuracy': final_acc.item(),
        'overall_accuracy': overall_acc.item(),
        'input_dim': input_dim,
        'hidden_dim': hidden_dim,
        'num_classes': num_classes,
        'training_args': {
            'epochs': epochs,
            'lr': lr,
            'hidden_dim': hidden_dim
        }
    }, model_path)
    print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")

    # ä¿å­˜æ³¨æ„åŠ›æƒé‡ç”¨äºåˆ†æ
    try:
        attention_weights = model.get_attention_weights(x)
        import numpy as np
        np.save('checkpoints/attention_weights.npy', attention_weights)
        print(f"ğŸ’¾ æ³¨æ„åŠ›æƒé‡å·²ä¿å­˜: checkpoints/attention_weights.npy")

        # æ–°å¢ï¼šå¯è§†åŒ–æ³¨æ„åŠ›æƒé‡
        from utils import plot_attention_weights
        plot_attention_weights(attention_weights, 'plots/attention_heatmap.png')

        # æ‰“å°æ³¨æ„åŠ›æƒé‡çš„ç»Ÿè®¡ä¿¡æ¯
        print(f"ğŸ“Š æ³¨æ„åŠ›æƒé‡ç»Ÿè®¡:")
        print(f"   - æœ€å°å€¼: {attention_weights.min():.6f}")
        print(f"   - æœ€å¤§å€¼: {attention_weights.max():.6f}")
        print(f"   - å¹³å‡å€¼: {attention_weights.mean():.6f}")
        print(f"   - æ ‡å‡†å·®: {attention_weights.std():.6f}")

    except Exception as e:
        print(f"âš ï¸  ä¿å­˜æ³¨æ„åŠ›æƒé‡å¤±è´¥: {e}")

    return final_acc.item()


def run_parameter_analysis(data_file=None, hidden_dims=[64, 128, 256], learning_rates=[0.01, 0.001]):
    """è¿è¡Œå‚æ•°åˆ†æ"""
    print("=" * 60)
    print("ğŸ§ª å¼€å§‹å‚æ•°åˆ†æ")
    print("=" * 60)

    # ä½¿ç”¨é»˜è®¤æ•°æ®æ–‡ä»¶è·¯å¾„
    if data_file is None:
        data_file = DataLoader.DEFAULT_DATA_PATH

    if not os.path.exists(data_file):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
        return {}

    results = {}

    for hidden_dim in hidden_dims:
        for lr in learning_rates:
            print(f"\nğŸ” æµ‹è¯•å‚æ•°: hidden_dim={hidden_dim}, lr={lr}")
            accuracy = run_training(
                data_file=data_file,
                epochs=100,  # å‡å°‘è½®æ•°ä»¥åŠ å¿«åˆ†æ
                lr=lr,
                hidden_dim=hidden_dim
            )
            key = f"hidden_{hidden_dim}_lr_{lr}"
            results[key] = accuracy

    # æ˜¾ç¤ºåˆ†æç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ“Š å‚æ•°åˆ†æç»“æœ")
    print("=" * 60)
    for config, acc in sorted(results.items(), key=lambda x: x[1], reverse=True):
        print(f"ğŸ”§ {config:25} | å‡†ç¡®ç‡: {acc:.4f}")
    print("=" * 60)

    return results


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='è‡ªé€‚åº”GCNç–¾ç—…æ•°æ®åˆ†æç³»ç»Ÿ')
    parser.add_argument('--data-file', type=str, default=None,
                        help='ç–¾ç—…æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆCSVæ ¼å¼ï¼‰ï¼Œå¦‚æœä¸æŒ‡å®šåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„')
    parser.add_argument('--epochs', type=int, default=200, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--lr', type=float, default=0.01, help='å­¦ä¹ ç‡')
    parser.add_argument('--hidden-dim', type=int, default=128, help='éšè—å±‚ç»´åº¦')
    parser.add_argument('--analyze', action='store_true',
                        help='è¿è¡Œå‚æ•°åˆ†æï¼ˆæµ‹è¯•ä¸åŒéšè—å±‚ç»´åº¦å’Œå­¦ä¹ ç‡ï¼‰')

    args = parser.parse_args()

    if args.analyze:
        # è¿è¡Œå‚æ•°åˆ†æ
        run_parameter_analysis(args.data_file)
    else:
        # è¿è¡Œå•æ¬¡è®­ç»ƒ
        run_training(
            data_file=args.data_file,
            epochs=args.epochs,
            lr=args.lr,
            hidden_dim=args.hidden_dim
        )


if __name__ == "__main__":
    main()
